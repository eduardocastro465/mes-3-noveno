{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c7c378c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "import os\n",
    "import json\n",
    "\n",
    "# --- SECCI√ìN 1: Carga de Datos y Generaci√≥n/Carga de Reglas de Asociaci√≥n ---\n",
    "\n",
    "RULES_FILE = 'rules_recomendacion.json'\n",
    "DATA_FILE = 'atelier-dataset-2023-2025-transacciones-unificado.csv'\n",
    "df_completo = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cf0df4b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reglas de asociaci√≥n cargadas desde 'rules_recomendacion.json'.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    if os.path.exists(RULES_FILE):\n",
    "        rules_df = pd.read_json(RULES_FILE)\n",
    "        # Convertimos las columnas de listas (del JSON) a frozensets\n",
    "        rules_df['antecedents'] = rules_df['antecedents'].apply(frozenset)\n",
    "        rules_df['consequents'] = rules_df['consequents'].apply(frozenset)\n",
    "        print(f\"\\nReglas de asociaci√≥n cargadas desde '{RULES_FILE}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nNo se encontr√≥ el archivo de reglas '{RULES_FILE}'. Generando nuevas reglas...\")\n",
    "    # Si no existe el archivo de reglas, lo generamos desde cero\n",
    "    try:\n",
    "        df_completo = pd.read_csv(DATA_FILE)\n",
    "        print(f\"DataFrame '{DATA_FILE}' cargado exitosamente para la generaci√≥n de reglas.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontr√≥ el archivo de datos '{DATA_FILE}'. Usando datos simulados para demostraci√≥n.\")\n",
    "        # Datos simulados para demostraci√≥n\n",
    "        data_simulada = {\n",
    "            'transaccion_id': [f'T{i}' for i in range(1, 201)],\n",
    "            'producto_talla': np.random.choice(['S', 'M', 'L', 'Talla √önica'], size=200),\n",
    "            'producto_color': np.random.choice(['Rojo', 'Negro', 'Azul', 'Beige', 'Verde', 'Blanco'], size=200),\n",
    "            'producto_temporada': np.random.choice(['Verano', 'Invierno', 'Primavera', 'Todo el A√±o'], size=200),\n",
    "            'vestido_estilo': np.random.choice(['Casual', 'Noche', 'C√≥ctel', 'Bohemio'], size=200),\n",
    "            'vestido_condicion': np.random.choice(['Nuevo', 'Usado'], size=200),\n",
    "            'producto_nombre': ['Vestido ' + str(i) for i in range(1, 201)],\n",
    "            'rese√±a_rating': np.random.uniform(3.5, 5.0, size=200).round(1),\n",
    "            'vestido_precio_venta': np.random.randint(500, 2500, size=200)\n",
    "        }\n",
    "        df_completo = pd.DataFrame(data_simulada)\n",
    "        df_completo.loc[0:20, ['producto_talla', 'vestido_estilo']] = ['M', 'Casual']\n",
    "        df_completo.loc[0:20, 'producto_temporada'] = 'Verano'\n",
    "\n",
    "    # 2. Preprocesamiento de datos y generaci√≥n del 'basket'\n",
    "    atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "    df_completo.dropna(subset=['transaccion_id'] + atributos, inplace=True)\n",
    "    df_completo.reset_index(drop=True, inplace=True)\n",
    "    df_analisis = df_completo[['transaccion_id'] + atributos].copy()\n",
    "    list_of_attributes = [df_analisis[attr].apply(lambda x: f\"{attr}_{x}\") for attr in atributos]\n",
    "    items = pd.concat(list_of_attributes, ignore_index=True)\n",
    "    df_basket = pd.DataFrame({\n",
    "        'transaccion_id': df_analisis['transaccion_id'].repeat(len(atributos)).reset_index(drop=True),\n",
    "        'item': items\n",
    "    })\n",
    "    basket = (df_basket.groupby(['transaccion_id', 'item'])['item'].count().unstack(fill_value=0).reset_index().set_index('transaccion_id'))\n",
    "    basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # 3. Filtrar y aplicar Apriori para generar reglas\n",
    "    min_transactions = 5 \n",
    "    product_counts = basket.sum(axis=0)\n",
    "    products_to_keep = product_counts[product_counts >= min_transactions].index\n",
    "    basket_filtered = basket[products_to_keep]\n",
    "    basket_bool = basket_filtered.astype(bool)\n",
    "\n",
    "    try:\n",
    "        frequent_itemsets = apriori(basket_bool, min_support=0.01, use_colnames=True)\n",
    "        rules_df = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "        rules_df = rules_df.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
    "        print(f\"\\nSe generaron {len(rules_df)} reglas de asociaci√≥n.\")\n",
    "        # Preparamos el DataFrame para guardarlo en JSON\n",
    "        # Convertimos frozensets a listas para que puedan serializarse\n",
    "        rules_df['antecedents'] = rules_df['antecedents'].apply(list)\n",
    "        rules_df['consequents'] = rules_df['consequents'].apply(list)\n",
    "        rules_df.to_json(RULES_FILE, orient='records', indent=4)\n",
    "        print(f\"Reglas guardadas en '{RULES_FILE}'.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar reglas: {e}. Usando un DataFrame de reglas vac√≠o.\")\n",
    "        rules_df = pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5eed1eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_por_similitud(atributos_elegidos, rules_df, df_transacciones_para_fallback, num_recomendaciones=2):\n",
    "    \"\"\"\n",
    "    Busca la regla m√°s similar a los atributos dados, priorizando lift y confianza.\n",
    "    \"\"\"\n",
    "    if not rules_df.empty:\n",
    "        mejores_reglas = []\n",
    "        for _, row in rules_df.iterrows():\n",
    "            antecedent = row['antecedents']\n",
    "            coincidencias = len(antecedent.intersection(atributos_elegidos))\n",
    "            if coincidencias > 0:\n",
    "                mejores_reglas.append({\n",
    "                    'regla': row,\n",
    "                    'coincidencias': coincidencias\n",
    "                })\n",
    "        \n",
    "        if mejores_reglas:\n",
    "            mejores_reglas.sort(key=lambda x: (x['coincidencias'], x['regla']['lift'], x['regla']['confidence']), reverse=True)\n",
    "            mejor_regla = mejores_reglas[0]['regla']\n",
    "            recomendaciones = list(mejor_regla['consequents'])\n",
    "            atributos_finales = [item for item in recomendaciones if item not in atributos_elegidos]\n",
    "            \n",
    "            print(\"\\n--- Diagn√≥stico: Regla m√°s similar encontrada ---\")\n",
    "            print(f\"Antecedente: {set(mejor_regla['antecedents'])}\")\n",
    "            print(f\"Consecuente: {set(mejor_regla['consequents'])}\")\n",
    "            print(f\"Lift: {mejor_regla['lift']:.2f}, Confianza: {mejor_regla['confidence']:.2f}\")\n",
    "\n",
    "            return atributos_finales[:num_recomendaciones], \"regla_mas_similar\"\n",
    "\n",
    "    print(\"üí° No se encontraron reglas de asociaci√≥n. Usando el fallback de atributos populares.\")\n",
    "    columnas_atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "    list_of_attributes = [df_transacciones_para_fallback[attr].apply(lambda x: f\"{attr}_{x}\") \n",
    "                          for attr in columnas_atributos if attr in df_transacciones_para_fallback.columns and not df_transacciones_para_fallback[attr].empty]\n",
    "    if not list_of_attributes:\n",
    "        return [], \"no_hay_datos\"\n",
    "    atributos_populares_serie = pd.concat(list_of_attributes)\n",
    "    atributos_populares = (atributos_populares_serie.value_counts().head(num_recomendaciones + len(atributos_elegidos)).index.tolist())\n",
    "    general_popular_attributes = [attr for attr in atributos_populares if attr not in atributos_elegidos]\n",
    "    return general_popular_attributes[:num_recomendaciones], \"atributos_populares\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4559be8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Advertencia: El archivo de datos 'atelier-dataset-2023-2025-transacciones-unificado.csv' no se encontr√≥ para la b√∫squeda final. Usando datos simulados.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'data_simulada' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 5\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m----> \u001b[39m\u001b[32m5\u001b[39m     df_completo = \u001b[43mpd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mDATA_FILE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiasd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[39m, in \u001b[36mread_csv\u001b[39m\u001b[34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[39m\n\u001b[32m   1024\u001b[39m kwds.update(kwds_defaults)\n\u001b[32m-> \u001b[39m\u001b[32m1026\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiasd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[39m, in \u001b[36m_read\u001b[39m\u001b[34m(filepath_or_buffer, kwds)\u001b[39m\n\u001b[32m    619\u001b[39m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m620\u001b[39m parser = \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    622\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiasd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[39m, in \u001b[36mTextFileReader.__init__\u001b[39m\u001b[34m(self, f, engine, **kwds)\u001b[39m\n\u001b[32m   1619\u001b[39m \u001b[38;5;28mself\u001b[39m.handles: IOHandles | \u001b[38;5;28;01mNone\u001b[39;00m = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1620\u001b[39m \u001b[38;5;28mself\u001b[39m._engine = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiasd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[39m, in \u001b[36mTextFileReader._make_engine\u001b[39m\u001b[34m(self, f, engine)\u001b[39m\n\u001b[32m   1879\u001b[39m         mode += \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1880\u001b[39m \u001b[38;5;28mself\u001b[39m.handles = \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1881\u001b[39m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1882\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1883\u001b[39m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1884\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcompression\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1885\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mmemory_map\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1886\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1887\u001b[39m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mencoding_errors\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstrict\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1888\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43moptions\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mstorage_options\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1889\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1890\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.handles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\hiasd\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[39m, in \u001b[36mget_handle\u001b[39m\u001b[34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[39m\n\u001b[32m    871\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ioargs.encoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs.mode:\n\u001b[32m    872\u001b[39m     \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m873\u001b[39m     handle = \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[32m    874\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    875\u001b[39m \u001b[43m        \u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    876\u001b[39m \u001b[43m        \u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m=\u001b[49m\u001b[43mioargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    877\u001b[39m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[43m=\u001b[49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    878\u001b[39m \u001b[43m        \u001b[49m\u001b[43mnewline\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    879\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    880\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    881\u001b[39m     \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [Errno 2] No such file or directory: 'atelier-dataset-2023-2025-transacciones-unificado.csv'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      6\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m:\n\u001b[32m      7\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mAdvertencia: El archivo de datos \u001b[39m\u001b[33m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mDATA_FILE\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m'\u001b[39m\u001b[33m no se encontr√≥ para la b√∫squeda final. Usando datos simulados.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m     df_completo = pd.DataFrame(\u001b[43mdata_simulada\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'data_simulada' is not defined"
     ]
    }
   ],
   "source": [
    "# --- SECCI√ìN 3: PRUEBA CON TU TRANSACCI√ìN ESPEC√çFICA ---\n",
    "# Cargamos el DataFrame completo para la funci√≥n de b√∫squeda de vestidos y fallback\n",
    "if df_completo is None:\n",
    "    try:\n",
    "        df_completo = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nAdvertencia: El archivo de datos '{DATA_FILE}' no se encontr√≥ para la b√∫squeda final. Usando datos simulados.\")\n",
    "        df_completo = pd.DataFrame(data_simulada)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52be2fb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    frequent_itemsets = apriori(basket_bool, min_support=0.01, use_colnames=True)\n",
    "    rules_df = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "    rules_df = rules_df.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
    "    rules_df['antecedents'] = rules_df['antecedents'].apply(list)\n",
    "    rules_df['consequents'] = rules_df['consequents'].apply(list)\n",
    "    rules_df.to_json(RULES_FILE, orient='records', indent=4)\n",
    "    print(f\"Reglas guardadas en '{RULES_FILE}'.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar reglas: {e}\")\n",
    "    rules_df = pd.DataFrame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccecbde2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def recomendar_por_similitud(atributos_elegidos, rules_df, df_transacciones_para_fallback, num_recomendaciones=2):\n",
    "    if not rules_df.empty:\n",
    "        mejores_reglas = []\n",
    "        for _, row in rules_df.iterrows():\n",
    "            antecedent = frozenset(row['antecedents'])\n",
    "            coincidencias = len(antecedent.intersection(atributos_elegidos))\n",
    "            if coincidencias > 0:\n",
    "                mejores_reglas.append({\n",
    "                    'regla': row,\n",
    "                    'coincidencias': coincidencias\n",
    "                })\n",
    "            if mejores_reglas:\n",
    "                mejores_reglas.sort(key=lambda x: (x['coincidencias'], x['regla']['lift'], x['regla']['confidence']), reverse=True)\n",
    "                mejor_regla = mejores_reglas[0]['regla']\n",
    "                recomendaciones = [item for item in mejor_regla['consequents'] if item not in atributos_elegidos]\n",
    "                print(\"\\n--- Regla m√°s similar encontrada ---\")\n",
    "                print(f\"Antecedente: {mejor_regla['antecedents']}\")\n",
    "                print(f\"Consecuente: {mejor_regla['consequents']}\")\n",
    "                return recomendaciones[:num_recomendaciones], \"regla_mas_similar\"\n",
    "            print(\"‚ö†Ô∏è No se encontraron reglas relevantes. Usando atributos populares.\")\n",
    "            columnas_atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "            list_of_attributes = [\n",
    "                df_transacciones_para_fallback[attr].apply(lambda x: f\"{attr}_{x}\")\n",
    "                for attr in columnas_atributos if attr in df_transacciones_para_fallback.columns\n",
    "          ]\n",
    "            if not list_of_attributes:\n",
    "                return [], \"no_hay_datos\"\n",
    "            atributos_populares = pd.concat(list_of_attributes).value_counts().index.tolist()\n",
    "            return [a for a in atributos_populares if a not in atributos_elegidos][:num_recomendaciones], \"atributos_populares\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f41742b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encontrar_vestidos_por_atributos(df_original, atributos_recomendados, excluidos=None):\n",
    "    if not atributos_recomendados: return pd.DataFrame()\n",
    "    filtros = []\n",
    "    columnas_mapeo = {\n",
    "        'producto_talla': 'producto_talla',\n",
    "        'producto_color': 'producto_color',\n",
    "        'producto_temporada': 'producto_temporada',\n",
    "        'vestido_estilo': 'vestido_estilo',\n",
    "        'vestido_condicion': 'vestido_condicion'\n",
    "    }\n",
    "    for attr_str in atributos_recomendados:\n",
    "        for prefix, col_name in columnas_mapeo.items():\n",
    "            if attr_str.startswith(f\"{prefix}_\"):\n",
    "                valor = attr_str.replace(f\"{prefix}_\", \"\", 1)\n",
    "                filtros.append(df_original[col_name] == valor)\n",
    "                break\n",
    "    if not filtros: return pd.DataFrame()\n",
    "    filtro_combinado = filtros[0]\n",
    "    for f in filtros[1:]:\n",
    "        filtro_combinado &= f\n",
    "    resultados = df_original[filtro_combinado].copy()\n",
    "    if excluidos:\n",
    "        resultados = resultados[~resultados['producto_nombre'].isin(excluidos)]\n",
    "    return resultados.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72a68dd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "if df_completo is None:\n",
    "    try:\n",
    "        df_completo = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nAdvertencia: El archivo de datos '{DATA_FILE}' no se encontr√≥ para la b√∫squeda final. Usando datos simulados.\")\n",
    "        df_completo = pd.DataFrame(data_simulada)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- PRUEBA DEL SISTEMA CON LA TRANSACCI√ìN QUE ME DISTE ---\")\n",
    "\n",
    "datos_transaccion = {\n",
    "    'producto_nombre': 'Vestido de Noche Meirius Largo Verde con Hombro Descubierto',\n",
    "    'producto_talla': 'XL',\n",
    "    'producto_color': 'verde',\n",
    "    'producto_temporada': 'Todo el A√±o',\n",
    "    'vestido_estilo': 'De Noche',\n",
    "    'vestido_condicion': 'Nuevo',\n",
    "}\n",
    "\n",
    "atributos_elegidos = frozenset([\n",
    "    f\"producto_talla_{datos_transaccion['producto_talla']}\",\n",
    "    f\"producto_color_{datos_transaccion['producto_color']}\",\n",
    "    f\"producto_temporada_{datos_transaccion['producto_temporada']}\",\n",
    "    f\"vestido_estilo_{datos_transaccion['vestido_estilo']}\",\n",
    "    f\"vestido_condicion_{datos_transaccion['vestido_condicion']}\"\n",
    "])\n",
    "\n",
    "print(f\"La transacci√≥n del cliente incluye un vestido con estos atributos:\\n{list(atributos_elegidos)}\")\n",
    "atributos_recomendados, metodo = recomendar_por_similitud(\n",
    "    atributos_elegidos=atributos_elegidos,\n",
    "    rules_df=rules_df,\n",
    "    df_transacciones_para_fallback=df_completo,\n",
    "    num_recomendaciones=3\n",
    ")\n",
    "\n",
    "print(f\"\\nEl sistema recomienda buscar vestidos con los siguientes atributos:\\n{atributos_recomendados}\")\n",
    "print(f\"(M√©todo utilizado: {metodo})\")\n",
    "\n",
    "vestidos_recomendados = encontrar_vestidos_por_atributos(\n",
    "    df_original=df_completo,\n",
    "    atributos_recomendados=atributos_recomendados,\n",
    "    excluidos=[datos_transaccion['producto_nombre']]\n",
    ")\n",
    "\n",
    "if not vestidos_recomendados.empty:\n",
    "    print(\"\\n¬°Vestidos recomendados para el cliente! (coinciden con los atributos sugeridos)\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    \n",
    "    columnas_a_mostrar = [\n",
    "        'producto_nombre',\n",
    "        'rese√±a_rating',\n",
    "        'producto_temporada',\n",
    "        'producto_talla',\n",
    "        'producto_color',\n",
    "        'vestido_estilo',\n",
    "        'vestido_condicion',\n",
    "        'vestido_precio_venta'\n",
    "    ]\n",
    "    \n",
    "    columnas_existentes = [col for col in columnas_a_mostrar if col in vestidos_recomendados.columns]\n",
    "    \n",
    "    df_final_recomendaciones = vestidos_recomendados[columnas_existentes]\n",
    "    print(df_final_recomendaciones)\n",
    "else:\n",
    "    print(\"\\nLo siento, no se encontraron vestidos en el cat√°logo que coincidan con los atributos recomendados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c7a6ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# --- TRANSACCI√ìN DE PRUEBA ---\n",
    "datos_transaccion = {\n",
    "    'producto_nombre': 'Vestido de Noche Meirius Largo Verde con Hombro Descubierto',\n",
    "    'producto_talla': 'XL',\n",
    "    'producto_color': 'verde',\n",
    "    'producto_temporada': 'Todo el A√±o',\n",
    "    'vestido_estilo': 'De Noche',\n",
    "    'vestido_condicion': 'Nuevo',\n",
    "}\n",
    "\n",
    "atributos_elegidos = frozenset([\n",
    "    f\"producto_talla_{datos_transaccion['producto_talla']}\",\n",
    "    f\"producto_color_{datos_transaccion['producto_color']}\",\n",
    "    f\"producto_temporada_{datos_transaccion['producto_temporada']}\",\n",
    "    f\"vestido_estilo_{datos_transaccion['vestido_estilo']}\",\n",
    "    f\"vestido_condicion_{datos_transaccion['vestido_condicion']}\"\n",
    "])\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"üéØ Atributos del vestido elegido:\")\n",
    "print(list(atributos_elegidos))\n",
    "\n",
    "atributos_recomendados, metodo = recomendar_por_similitud(\n",
    "    atributos_elegidos=atributos_elegidos,\n",
    "    rules_df=rules_df,\n",
    "    df_transacciones_para_fallback=df_completo,\n",
    "    num_recomendaciones=3\n",
    ")\n",
    "\n",
    "print(f\"\\nüîé Atributos recomendados: {atributos_recomendados} (M√©todo: {metodo})\")\n",
    "\n",
    "vestidos_recomendados = encontrar_vestidos_por_atributos(\n",
    "    df_original=df_completo,\n",
    "    atributos_recomendados=atributos_recomendados,\n",
    "    excluidos=[datos_transaccion['producto_nombre']]\n",
    ")\n",
    "\n",
    "if not vestidos_recomendados.empty:\n",
    "    print(\"\\n‚úÖ Vestidos recomendados:\")\n",
    "    columnas_a_mostrar = [\n",
    "        'producto_id',\n",
    "        'producto_nombre',\n",
    "        'rese√±a_rating',\n",
    "        'producto_temporada',\n",
    "        'producto_talla',\n",
    "        'producto_color',\n",
    "        'vestido_estilo',\n",
    "        'vestido_condicion',\n",
    "        'vestido_precio_venta'\n",
    "    ]\n",
    "    columnas_existentes = [col for col in columnas_a_mostrar if col in vestidos_recomendados.columns]\n",
    "    df_final_recomendaciones = vestidos_recomendados[columnas_existentes]\n",
    "    print(df_final_recomendaciones)\n",
    "\n",
    "    vestidos_json = df_final_recomendaciones.to_dict(orient='records')\n",
    "\n",
    "    joblib.dump(rules_df, 'rules_model.pkl')\n",
    "    joblib.dump(frequent_itemsets, 'frequent_itemsets.pkl')\n",
    "    joblib.dump(basket_filtered, 'basket_filtered.pkl')\n",
    "    print(\"‚úÖ Modelos exportados como archivos .pkl\")\n",
    "    print(\"\\nüì¶ Formato JSON:\")\n",
    "    print(json.dumps(vestidos_json, indent=4, ensure_ascii=False))\n",
    "else:\n",
    "    print(\"\\n‚ùå No se encontraron vestidos recomendados con los atributos sugeridos.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
