{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e5db8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "import scipy.cluster.hierarchy as sch\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import joblib\n",
    "from mlxtend.frequent_patterns import apriori, association_rules"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decfcf34",
   "metadata": {},
   "source": [
    "# Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0c2ba4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usar el archivo correcto definido en DATA_FILE\n",
    "data_df = pd.read_csv('atelier-datshet-2024-transacciones.csv')\n",
    "df = pd.DataFrame(data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddf140f1-ec3c-45cb-a7d7-5a7b948af560",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c71f1f-c996-4245-bc0d-86bcd856efa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "    # print(\"Informaci√≥n inicial del DataFrame:\")\n",
    "    # df.info()# 1. Filtrar solo las columnas necesarias para reglas de asociaci√≥n\n",
    "    # df_rules = df[['transaccion_id', 'producto_nombre', 'cantidad_comprada']]\n",
    "    \n",
    "    # # 2. Crear la \"cesta\": filas = transacci√≥n, columnas = producto, valores = cantidad total\n",
    "    # basket = df_rules.groupby(['transaccion_id', 'producto_nombre'])['cantidad_comprada'].sum().unstack().fillna(0)\n",
    "    \n",
    "    # # 3. Convertir a binario: 1 = comprado, 0 = no comprado\n",
    "    # basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "    \n",
    "    # # Mostrar las primeras filas\n",
    "    # print(\"üß∫ Matriz lista para Apriori:\")\n",
    "    # print(basket.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68dd980a-442c-42ae-9070-02028a0a9739",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 4. Preprocesamiento de datos\n",
    "df.dropna(subset=['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion'], inplace=True)\n",
    "df['cantidad_comprada'] = df['cantidad_comprada'].astype(int)\n",
    "\n",
    "# 6. Preparar datos para an√°lisis de asociaci√≥n\n",
    "atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "df_analisis = df[['transaccion_id'] + atributos].copy()\n",
    "\n",
    "list_of_attributes = []\n",
    "for attr in atributos:\n",
    "    list_of_attributes.append(df_analisis[attr].apply(lambda x: f\"{attr}_{x}\"))\n",
    "\n",
    "items = pd.concat(list_of_attributes, ignore_index=True)\n",
    "\n",
    "df_basket = pd.DataFrame({\n",
    "    'transaccion_id': df_analisis['transaccion_id'].repeat(len(atributos)).reset_index(drop=True),\n",
    "    'item': items\n",
    "})\n",
    "\n",
    "basket = (df_basket.groupby(['transaccion_id', 'item'])['item']\n",
    "          .count()\n",
    "          .unstack(fill_value=0)\n",
    "          .reset_index()\n",
    "          .set_index('transaccion_id'))\n",
    "\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# 1. Filtrar productos poco frecuentes (atributos)\n",
    "min_transactions = 2 # Ajustado para el ejemplo\n",
    "product_counts = basket.sum(axis=0)\n",
    "products_to_keep = product_counts[product_counts >= min_transactions].index\n",
    "basket_filtered = basket[products_to_keep]\n",
    "\n",
    "# 2. Convertir a booleanos expl√≠citamente\n",
    "basket_bool = basket_filtered.astype(bool)\n",
    "\n",
    "# 3. Aplicar Apriori con par√°metros optimizados\n",
    "try:\n",
    "    frequent_itemsets = apriori(basket_bool,\n",
    "                                min_support=0.05, # min_support ajustado para el ejemplo\n",
    "                                use_colnames=True,\n",
    "                                low_memory=True,\n",
    "                                max_len=3,\n",
    "                                verbose=1)\n",
    "\n",
    "    print(f\"\\nItemsets frecuentes encontrados: {len(frequent_itemsets)}\")\n",
    "\n",
    "    # 4. Generar y analizar reglas con filtros estrictos\n",
    "    if not frequent_itemsets.empty:\n",
    "        rules = association_rules(frequent_itemsets,\n",
    "                                 metric=\"lift\",\n",
    "                                 min_threshold=1.5)\n",
    "\n",
    "        print(f\"\\nReglas generadas: {len(rules)}\")\n",
    "\n",
    "        # --- AHORA VAMOS A MOSTRAR LAS REGLAS DE FORMA CLARA ---\n",
    "\n",
    "        # 1. Filtrar las reglas m√°s fuertes (lift > 1.5, confidence > 0.6)\n",
    "        strong_rules = rules[(rules['lift'] > 1.5) & (rules['confidence'] > 0.6)]\n",
    "        \n",
    "        # 2. Ordenar las reglas por 'lift' de forma descendente\n",
    "        strong_rules = strong_rules.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
    "        \n",
    "        if not strong_rules.empty:\n",
    "            print(\"\\n--- Las 5 reglas de asociaci√≥n m√°s fuertes (ordenadas por lift) ---\")\n",
    "            # Mostrar solo las columnas m√°s importantes para la interpretaci√≥n\n",
    "            print(strong_rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].head())\n",
    "        else:\n",
    "            print(\"No se encontraron reglas fuertes con los filtros de lift > 1.5 y confidence > 0.6.\")\n",
    "            print(\"Aqu√≠ est√°n las primeras 5 reglas generadas (sin filtros adicionales):\")\n",
    "            print(rules[['antecedents', 'consequents', 'support', 'confidence', 'lift']].sort_values(by='lift', ascending=False).head())\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo se encontraron itemsets frecuentes con los par√°metros actuales.\")\n",
    "\n",
    "except MemoryError:\n",
    "    print(\"Error de memoria. Intenta con:\")\n",
    "    print(\"- Un min_support m√°s alto\")\n",
    "    print(\"- Un max_len m√°s bajo\")\n",
    "    print(\"- Filtrar m√°s productos poco frecuentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95f00daf",
   "metadata": {},
   "source": [
    "# --- An√°lisis de Dispersi√≥n Inicial ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a40a4fef",
   "metadata": {},
   "source": [
    "# Seleccionar las columnas num√©ricas para el gr√°fico de dispersi√≥n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37944edd-1624-485f-a6af-1939e29fd795",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_scatter = df[['cantidad_comprada', 'monto_total']].values\n",
    "print(\"\\nPrimeras 5 filas de los datos para el gr√°fico de dispersi√≥n (Cantidad Comprada, Monto Total):\")\n",
    "print(X_scatter[0:5, :])\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_scatter[:, 0], X_scatter[:, 1], s=100, c='blue', alpha=0.6, label='Datos de Transacci√≥n')\n",
    "plt.xlabel('Cantidad de Productos Comprados por Transacci√≥n')\n",
    "plt.ylabel('Monto Total Gastado por Transacci√≥n')\n",
    "plt.title('Dispersi√≥n de Cantidad de Productos vs. Monto Total por Transacci√≥n')\n",
    "plt.legend()\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af72b90d",
   "metadata": {},
   "source": [
    "# --- M√©todo del Codo para K-Means ---\n",
    "# Calcular la inercia para diferentes n√∫meros de clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec46f28-c3fd-4f37-a472-825bd007c9cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "inercia = []\n",
    "for i in range(1, 11):\n",
    "    km = KMeans(n_clusters=i, init='random', max_iter=300, n_init=10, tol=1e-04, random_state=0)\n",
    "    km.fit(X_scatter)\n",
    "    inercia.append(km.inertia_)\n",
    "\n",
    "# Graficar el m√©todo del codo\n",
    "plt.figure(figsize=(10, 6)) # Ajustado el tama√±o para mejor visualizaci√≥n\n",
    "plt.plot(range(1, 11), inercia, marker='o')\n",
    "plt.title('M√©todo del Codo')\n",
    "plt.xlabel('N√∫mero de clusters')\n",
    "plt.ylabel('Inercia')\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea1963",
   "metadata": {},
   "source": [
    "# --- Dendrograma para Agrupamiento Jer√°rquico ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d625b71-a430-46dc-b08f-9969b51d1cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar las columnas num√©ricas relevantes para el clustering jer√°rquico\n",
    "X_dendrogram = df[['cantidad_comprada', 'monto_total']].values\n",
    "\n",
    "# Convertir a DataFrame para muestreo\n",
    "X_dendrogram_df = pd.DataFrame(X_dendrogram, columns=['cantidad_comprada', 'monto_total'])\n",
    "\n",
    "# Determinar el tama√±o de la muestra (para datasets grandes)\n",
    "sample_size = min(500, len(X_dendrogram_df))\n",
    "muestra = X_dendrogram_df.sample(n=sample_size, random_state=0).values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f258f935",
   "metadata": {},
   "source": [
    "\n",
    "# Crear el dendrograma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "407cfcce",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "dendrogram = sch.dendrogram(sch.linkage(muestra, method='ward'))\n",
    "plt.title('Dendrograma de Transacciones por Cantidad y Monto')\n",
    "plt.xlabel(f'Puntos de Datos (Muestra de {sample_size} Transacciones)')\n",
    "plt.ylabel('Distancia Euclidiana')\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.show()\n",
    "print(f\"Dendrograma generado a partir de una muestra de {sample_size} puntos de datos.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2603f599",
   "metadata": {},
   "source": [
    "# --- Aplicar K-Means con el n√∫mero √≥ptimo de clusters (ej. 3, basado en el m√©todo del codo) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58f79398-b414-4dac-8ac7-17be9eef5c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Apply K-Means and assign transaction-level clusters to the original 'df'\n",
    "num_clusters_kmeans = 3 # Adjust based on your elbow method\n",
    "km = KMeans(n_clusters=num_clusters_kmeans, init='random', max_iter=300, n_init=10, tol=1e-04, random_state=0)\n",
    "y_km = km.fit_predict(X_scatter) # X_scatter contains 'cantidad_comprada', 'monto_total' for transactions\n",
    "df['cluster'] = y_km # This adds 'cluster' to your transaction DataFrame 'df'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a973de93",
   "metadata": {},
   "source": [
    "# Visualizar los clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76ff5a8-109d-4661-aa01-10827a7da33d",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X_scatter[y_km == 0, 0], X_scatter[y_km == 0, 1], s=100, c='red', label='Cluster 1')\n",
    "plt.scatter(X_scatter[y_km == 1, 0], X_scatter[y_km == 1, 1], s=100, c='blue', label='Cluster 2')\n",
    "plt.scatter(X_scatter[y_km == 2, 0], X_scatter[y_km == 2, 1], s=100, c='green', label='Cluster 3')\n",
    "plt.xlabel('Cantidad de Productos Comprados') # Ajustado a los datos usados\n",
    "plt.ylabel('Monto Total Gastado') # Ajustado a los datos usados\n",
    "plt.title(f'Clusters de Transacciones (K-Means, {num_clusters_kmeans} Clusters)')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33eac090",
   "metadata": {},
   "source": [
    "# Asignar los resultados del clustering a una nueva columna 'cluster' en el DataFrame 'df'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e23692",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cluster'] = y_km"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fa4d79d",
   "metadata": {},
   "source": [
    "# Obtener los clientes de los diferentes clusters (ejemplo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906110b3-fb95-4011-b2a4-2dc12210437d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Clientes por Cluster (Muestra) ---\")\n",
    "cluster_0_clientes = df[df['cluster'] == 0]['cliente_nombre'].head()\n",
    "cluster_1_clientes = df[df['cluster'] == 1]['cliente_nombre'].head()\n",
    "cluster_2_clientes = df[df['cluster'] == 2]['cliente_nombre'].head()\n",
    "\n",
    "print(\"\\nClientes del Cl√∫ster 0 (primeros 5):\")\n",
    "print(cluster_0_clientes)\n",
    "print(\"\\nClientes del Cl√∫ster 1 (primeros 5):\")\n",
    "print(cluster_1_clientes)\n",
    "print(\"\\nClientes del Cl√∫ster 2 (primeros 5):\")\n",
    "print(cluster_2_clientes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163b1c34",
   "metadata": {},
   "source": [
    "# --- Crear DataFrame de Clientes Agregado ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "130e2f77-fbf5-4057-8e36-c45cfb02d1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_clientes = df.groupby('cliente_nombre').agg(\n",
    "    num_transacciones=('transaccion_id', 'nunique'),\n",
    "    monto_total_gastado=('monto_total', 'sum'),\n",
    "    # Asumimos que un cliente cae en un solo cl√∫ster.\n",
    "    # Si un cliente aparece en m√∫ltiples cl√∫steres debido a c√≥mo se hizo el clustering de transacciones,\n",
    "    # esto tomar√° el primer cl√∫ster que encuentre para ese cliente.\n",
    "    # Una aproximaci√≥n m√°s robusta ser√≠a re-clusterizar a nivel de cliente.\n",
    "    cluster=('cluster', lambda x: x.mode()[0]) # Tomar el cl√∫ster m√°s frecuente si un cliente tiene m√∫ltiples\n",
    ").reset_index()\n",
    "\n",
    "print(\"\\nDataFrame 'df_clientes' agregado (primeras 5 filas):\")\n",
    "print(df_clientes.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccb78fa6",
   "metadata": {},
   "source": [
    "# --- Preparaci√≥n de Datos para el Clasificador ---\n",
    "# Caracter√≠sticas (X) y variable objetivo (y) para el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ae376bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_model = df_clientes[['num_transacciones', 'monto_total_gastado']]\n",
    "y_model = df_clientes['cluster']\n",
    "\n",
    "print(\"\\nPrimeras filas de X (caracter√≠sticas para el modelo):\")\n",
    "print(X_model.head())\n",
    "print(\"\\nPrimeras filas de y (variable objetivo - cl√∫steres):\")\n",
    "print(y_model.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85f2eef9",
   "metadata": {},
   "source": [
    "\n",
    "# Escalar las caracter√≠sticas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6473f93-f424-4e68-af62-7adb16b22a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_model)\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=['num_transacciones', 'monto_total_gastado'])\n",
    "\n",
    "print(\"\\nDataFrame de Caracter√≠sticas Escaladas (X_scaled_df - primeras 5 filas):\")\n",
    "print(X_scaled_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acc6f9d2",
   "metadata": {},
   "source": [
    "# --- Entrenamiento y Evaluaci√≥n del Clasificador ---\n",
    "# Dividir los datos en conjunto de entrenamiento y prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "141d1b60-deb4-4947-b7de-cf9fd39e4f52",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X_scaled_df, y_model, test_size=0.2, random_state=0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80c7bbf7",
   "metadata": {},
   "source": [
    "# Entrenar el clasificador (RandomForestClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef4dafb7-dd16-4fe3-bf57-c70d582ca2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = RandomForestClassifier(n_estimators=100, random_state=0)\n",
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11d287f8-70e7-4e7f-8472-14956a63cb3b",
   "metadata": {},
   "source": [
    "# Predecir en el conjunto de prueba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9812ff7f-404d-49d2-8d2e-b292b9f9c03f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d64ec0be",
   "metadata": {},
   "source": [
    "# Evaluar el clasificador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6db0d7c7-f216-4f9f-9ada-6d86e426f61b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Evaluaci√≥n del Clasificador ---\")\n",
    "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
    "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e391478c",
   "metadata": {},
   "source": [
    "# Opcional: Guardar el clasificador entrenado y el escalador"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de156fde-75d3-4145-977e-4b216d61156b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "joblib.dump(clf, 'customer_cluster_classifier_atlier.pkl')\n",
    "joblib.dump(scaler, 'scaler_atlier.pkl')\n",
    "print(\"\\nClasificador y escalador guardados como 'customer_cluster_classifier.pkl' y 'scaler.pkl'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78907ec9",
   "metadata": {},
   "source": [
    "\n",
    "# --- PASO ADICIONAL DEL USUARIO: Cargar y preparar para nuevas transacciones y recomendaciones ---\n",
    "\n",
    "# --- Cargar el modelo y escalador (si est√°s en un nuevo script o sesi√≥n) ---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a487b3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- FUNCIONES DE RECOMENDACI√ìN (ACTUALIZADAS PARA INCLUIR REGLAS) ---\n",
    "\n",
    "# Aseg√∫rate de que 'rules' est√© definido antes de usarlo\n",
    "try:\n",
    "    rules\n",
    "except NameError:\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    try:\n",
    "        rules = joblib.load('rules.pkl')\n",
    "        print(\"Reglas de asociaci√≥n cargadas exitosamente desde 'rules.pkl'.\")\n",
    "    except Exception:\n",
    "        print(\"Advertencia: 'rules' no est√° definido y no se pudo cargar 'rules.pkl'. Se usar√° un DataFrame vac√≠o.\")\n",
    "        rules = pd.DataFrame(columns=['antecedents', 'consequents', 'confidence'])\n",
    "\n",
    "def obtener_productos_top_frecuencia(cluster_id, df_transacciones_con_cluster_local, num_productos=3):\n",
    "    \"\"\"Obtiene los productos m√°s frecuentes para un cl√∫ster dado.\"\"\"\n",
    "    # Usar 'cluster_x' para filtrar\n",
    "    if cluster_id not in df_transacciones_con_cluster_local['cluster_x'].unique():\n",
    "        return []\n",
    "    cluster_df_filtered = df_transacciones_con_cluster_local[df_transacciones_con_cluster_local['cluster_x'] == cluster_id]\n",
    "    top_productos = cluster_df_filtered['producto_nombre'].value_counts().head(num_productos)\n",
    "    return top_productos.index.tolist()\n",
    "\n",
    "def obtener_productos_top_monto(cluster_id, df_transacciones_con_cluster_local, num_productos=3):\n",
    "    \"\"\"Obtiene los productos que generan mayor monto para un cl√∫ster dado.\"\"\"\n",
    "    # Usar 'cluster_x' para filtrar\n",
    "    if cluster_id not in df_transacciones_con_cluster_local['cluster_x'].unique():\n",
    "        return []\n",
    "    cluster_df_filtered = df_transacciones_con_cluster_local[df_transacciones_con_cluster_local['cluster_x'] == cluster_id]\n",
    "    top_monto_productos = cluster_df_filtered.groupby('producto_nombre')['monto_total'].sum().nlargest(num_productos)\n",
    "    return top_monto_productos.index.tolist()\n",
    "\n",
    "def recomendar_por_reglas_asociacion(producto_elegido, rules_df, df_transacciones_para_fallback, nuevo_cluster_cliente, num_recomendaciones=2):\n",
    "    \"\"\"\n",
    "    Recomienda productos basados en reglas de asociaci√≥n dado un producto ya elegido.\n",
    "    Si no encuentra reglas directas, ofrece alternativas basadas en el cl√∫ster del cliente\n",
    "    o en los productos m√°s populares en general.\n",
    "    \"\"\"\n",
    "    recomendaciones_reglas = set()\n",
    "    \n",
    "    # Intenta encontrar recomendaciones directas por reglas de asociaci√≥n\n",
    "    for _, row in rules_df.iterrows():\n",
    "        if producto_elegido in row['antecedents']:\n",
    "            for item in row['consequents']:\n",
    "                if item != producto_elegido: # Asegurarse de no recomendar el mismo producto\n",
    "                    recomendaciones_reglas.add(item)\n",
    "                    if len(recomendaciones_reglas) >= num_recomendaciones:\n",
    "                        return list(recomendaciones_reglas)[:num_recomendaciones], \"reglas_directas\"\n",
    "    \n",
    "    # Si no hay suficientes recomendaciones por reglas directas, busca alternativas\n",
    "    if len(recomendaciones_reglas) < num_recomendaciones:\n",
    "        print(\"üí° No se encontraron productos complementarios directos por reglas de asociaci√≥n.\")\n",
    "        print(f\"Considera estos vestidos que suelen gustarle a clientes como t√∫ (Cl√∫ster {nuevo_cluster_cliente}):\")\n",
    "        \n",
    "        # Fallback 1: Productos TOP por frecuencia/monto del mismo cl√∫ster\n",
    "        # Esto es similar a la recomendaci√≥n por cl√∫ster principal, pero lo traemos aqu√≠ si las reglas fallan.\n",
    "        # Podr√≠as elegir entre frecuencia o monto dependiendo de tu estrategia para este fallback\n",
    "        fallback_sugerencias = obtener_productos_top_frecuencia(nuevo_cluster_cliente, df_transacciones_para_fallback, num_productos=num_recomendaciones)\n",
    "        \n",
    "        # Eliminar el producto base si est√° en las sugerencias de fallback\n",
    "        fallback_sugerencias = [prod for prod in fallback_sugerencias if prod != producto_elegido]\n",
    "\n",
    "        if fallback_sugerencias:\n",
    "            return fallback_sugerencias[:num_recomendaciones], \"cluster_fallback\"\n",
    "        else:\n",
    "            # Fallback 2: Productos m√°s populares en general si el cl√∫ster tampoco da resultados\n",
    "            print(\"Considera estos vestidos populares en el Atelier:\")\n",
    "            general_popular_products = df_transacciones_para_fallback['producto_nombre'].value_counts().head(num_recomendaciones + 1).index.tolist()\n",
    "            general_popular_products = [prod for prod in general_popular_products if prod != producto_elegido]\n",
    "            return general_popular_products[:num_recomendaciones], \"general_popular\"\n",
    "            \n",
    "    return list(recomendaciones_reglas), \"reglas_directas\" # Retorna lo que encontr√≥ por reglas si fue suficiente\n",
    "\n",
    "# --- PROCESO PARA UNA NUEVA TRANSACCI√ìN Y RECOMENDACI√ìN INTEGRADA ---\n",
    "\n",
    "# Cargar el clasificador entrenado y el escalador\n",
    "try:\n",
    "    clf_cargado = joblib.load('customer_cluster_classifier.pkl')\n",
    "    scaler_cargado = joblib.load('scaler.pkl')\n",
    "    print(\"\\nModelo de clasificador y escalador cargados exitosamente.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: Aseg√∫rate de que 'customer_cluster_classifier.pkl' y 'scaler.pkl' existen y est√°n en la ruta correcta.\")\n",
    "    exit()\n",
    "\n",
    "# Crear df_transacciones_con_cluster con la columna 'cluster'\n",
    "# ASUMO que 'df' y 'df_clientes' ya est√°n cargados y disponibles en este script.\n",
    "# Si no es as√≠, necesitar√≠as cargarlos aqu√≠ (ej. desde CSVs o la API).\n",
    "# Por ejemplo:\n",
    "# df = pd.read_csv('tu_archivo_de_transacciones_filtrado.csv') \n",
    "# df_clientes = pd.read_csv('tu_archivo_de_clientes_con_cluster.csv') \n",
    "\n",
    "df_transacciones_con_cluster = pd.merge(df, df_clientes[['cliente_nombre', 'cluster']],\n",
    "                                         on='cliente_nombre',\n",
    "                                         how='left')\n",
    "print(\"\\nVerificaci√≥n: Primeras filas de df_transacciones_con_cluster con la columna 'cluster':\")\n",
    "print(df_transacciones_con_cluster.head())\n",
    "\n",
    "# --- Simular la Nueva Transacci√≥n ---\n",
    "# Old transaction data\n",
    "nueva_transaccion = {\n",
    "    'transaccion_id': '687e0e6d2d8655b05b08c3a6',\n",
    "    'fecha_transaccion': '2024-10-28 12:25:01',\n",
    "    'tipo_transaccion': 'Venta',\n",
    "    'estado': 'Completada',\n",
    "    'monto_total': 3800,\n",
    "    'metodo_pago': 'tarjeta_credito',\n",
    "    'cliente_nombre': 'alejandra.paredes@example.com',\n",
    "    'cliente_telefono': 1234567827,\n",
    "    'producto_nombre': 'Vestido Largo de Seda Morado Obscuro',\n",
    "    'producto_talla': 'M',\n",
    "    'producto_color': 'Morado Oscuro',\n",
    "    'producto_temporada': 'Todo el A√±o',\n",
    "    'cantidad_comprada': 1,\n",
    "    'fecha_entrega': '2024-10-28 22:25:01',\n",
    "    'fecha_devolucion': None, # Using None for NaN as it's more appropriate for missing data in Python\n",
    "    'multa_aplicada': 0,\n",
    "    'rese√±a_rating': 5\n",
    "}\n",
    "\n",
    "\n",
    "df_nueva_transaccion = pd.DataFrame([nueva_transaccion])\n",
    "df_nueva_transaccion['fecha_transaccion'] = pd.to_datetime(df_nueva_transaccion['fecha_transaccion'])\n",
    "\n",
    "# --- Actualizar las M√©tricas del Cliente en df_clientes ---\n",
    "cliente_afectado = nueva_transaccion['cliente_nombre']\n",
    "\n",
    "if cliente_afectado in df_clientes['cliente_nombre'].values:\n",
    "    df_clientes.loc[df_clientes['cliente_nombre'] == cliente_afectado, 'num_transacciones'] += 1\n",
    "    df_clientes.loc[df_clientes['cliente_nombre'] == cliente_afectado, 'monto_total_gastado'] += nueva_transaccion['monto_total']\n",
    "    \n",
    "    print(f\"\\nM√©tricas actualizadas para '{cliente_afectado}':\")\n",
    "    print(df_clientes[df_clientes['cliente_nombre'] == cliente_afectado][['num_transacciones', 'monto_total_gastado']])\n",
    "else:\n",
    "    print(f\"\\nEl cliente '{cliente_afectado}' no encontrado en df_clientes. Se necesitar√≠a l√≥gica para agregar un nuevo cliente.\")\n",
    "\n",
    "# --- Obtener las M√©tricas ACTUALIZADAS del cliente para la re-clasificaci√≥n ---\n",
    "cliente_data_actualizada = df_clientes[df_clientes['cliente_nombre'] == cliente_afectado][\n",
    "    ['num_transacciones', 'monto_total_gastado']\n",
    "]\n",
    "\n",
    "if not cliente_data_actualizada.empty:\n",
    "    cliente_scaled_actualizado = scaler_cargado.transform(cliente_data_actualizada)\n",
    "    cliente_scaled_actualizado_df = pd.DataFrame(cliente_scaled_actualizado,\n",
    "                                                 columns=['num_transacciones', 'monto_total_gastado'])\n",
    "\n",
    "    # --- Predecir el NUEVO Cl√∫ster del Cliente ---\n",
    "    nuevo_cluster_predicho = clf_cargado.predict(cliente_scaled_actualizado_df)[0]\n",
    "    print(f\"\\nEl cliente '{cliente_afectado}' (con la nueva transacci√≥n) ahora pertenece al Cl√∫ster: {nuevo_cluster_predicho}\")\n",
    "\n",
    "    # --- Actualizar el cl√∫ster del cliente en df_clientes (si cambi√≥) ---\n",
    "    old_cluster = df_clientes.loc[df_clientes['cliente_nombre'] == cliente_afectado, 'cluster'].iloc[0]\n",
    "    if old_cluster != nuevo_cluster_predicho:\n",
    "        df_clientes.loc[df_clientes['cliente_nombre'] == cliente_afectado, 'cluster'] = nuevo_cluster_predicho\n",
    "        print(f\"El cl√∫ster de '{cliente_afectado}' ha cambiado de {old_cluster} a {nuevo_cluster_predicho}.\")\n",
    "    else:\n",
    "        print(f\"El cl√∫ster de '{cliente_afectado}' se mantiene en {old_cluster}.\")\n",
    "\n",
    "    # --- Generar Recomendaciones Integradas: POR CL√öSTER y POR REGLAS ---\n",
    "    print(\"\\n--- RECOMENDACIONES INTEGRADA DE PRODUCTOS ---\")\n",
    "\n",
    "    # 1. Recomendaci√≥n por Cl√∫ster (Perfil del Cliente)\n",
    "    print(f\"üî• Vestidos TOP para tu perfil (Cl√∫ster {nuevo_cluster_predicho}):\")\n",
    "    sugerencias_cluster = []\n",
    "    if nuevo_cluster_predicho == 0:\n",
    "        sugerencias_cluster = obtener_productos_top_monto(nuevo_cluster_predicho, df_transacciones_con_cluster, num_productos=3)\n",
    "        print(\"Sugerencias de productos (basadas en alto ingreso para el cl√∫ster):\")\n",
    "        print(\"Acci√≥n estrat√©gica: Ofr√©cele descuentos exclusivos o acceso anticipado a colecciones premium.\")\n",
    "    elif nuevo_cluster_predicho == 1:\n",
    "        sugerencias_cluster = obtener_productos_top_frecuencia(nuevo_cluster_predicho, df_transacciones_con_cluster, num_productos=3)\n",
    "        print(\"Sugerencias de productos (basadas en frecuencia para el cl√∫ster):\")\n",
    "        print(\"Acci√≥n estrat√©gica: Sugiere productos complementarios o programas de fidelidad para aumentar el valor promedio de su compra.\")\n",
    "    elif nuevo_cluster_predicho == 2:\n",
    "        sugerencias_cluster = obtener_productos_top_monto(nuevo_cluster_predicho, df_transacciones_con_cluster, num_productos=3)\n",
    "        print(\"Sugerencias de productos (basadas en alto ingreso para el cl√∫ster):\")\n",
    "        print(\"Acci√≥n estrat√©gica: Recu√©rdale las √∫ltimas novedades de lujo o productos de alta gama que suelen interesarle.\")\n",
    "    else:\n",
    "        print(\"Cl√∫ster no reconocido. Ofrecer recomendaciones generales.\")\n",
    "        sugerencias_cluster = df_transacciones_con_cluster['producto_nombre'].value_counts().head(3).index.tolist()\n",
    "\n",
    "    if sugerencias_cluster:\n",
    "        for prod in sugerencias_cluster:\n",
    "            print(f\"- {prod}\")\n",
    "    else:\n",
    "        print(\"No se encontraron sugerencias de productos para este cl√∫ster.\")\n",
    "\n",
    "    # 2. Recomendaci√≥n por Reglas de Asociaci√≥n (Productos Relacionados)\n",
    "    producto_base_para_reglas = nueva_transaccion['producto_nombre'] # El vestido que el cliente acaba de \"elegir\"\n",
    "\n",
    "    # **MODIFICACI√ìN AQU√ç: Pasar df_transacciones_con_cluster y el nuevo_cluster_predicho para los fallbacks**\n",
    "    sugerencias_reglas, tipo_sugerencia = recomendar_por_reglas_asociacion(\n",
    "        producto_base_para_reglas,\n",
    "        rules,\n",
    "        df_transacciones_con_cluster, # Datos completos para fallbacks de cl√∫ster/general\n",
    "        nuevo_cluster_predicho,       # Cl√∫ster actual del cliente\n",
    "        num_recomendaciones=3\n",
    "    )\n",
    "\n",
    "    if tipo_sugerencia == \"reglas_directas\":\n",
    "        print(f\"\\nüõçÔ∏è ¬°Perfecto! Combina tu '{producto_base_para_reglas}' con estos estilos, basados en lo que otros clientes tambi√©n eligen:\")\n",
    "        if sugerencias_reglas:\n",
    "            for prod_relacionado in sugerencias_reglas:\n",
    "                # Encuentra la confianza de la regla para mostrarla\n",
    "                antecedent_set = frozenset([producto_base_para_reglas])\n",
    "                consequent_set = frozenset([prod_relacionado])\n",
    "                matching_rule = rules[\n",
    "                    (rules['antecedents'] == antecedent_set) &\n",
    "                    (rules['consequents'] == consequent_set)\n",
    "                ]\n",
    "                confidence_str = \"\"\n",
    "                if not matching_rule.empty:\n",
    "                    confidence_str = f\" (Confianza: {matching_rule['confidence'].iloc[0]*100:.1f}%)\"\n",
    "                print(f\"- {prod_relacionado}{confidence_str}\")\n",
    "        else:\n",
    "            print(\"No se encontraron productos complementarios directos basados en reglas de asociaci√≥n.\")\n",
    "    elif tipo_sugerencia == \"cluster_fallback\":\n",
    "        print(f\"\\nüëó ¬°Te podr√≠a encantar! Ya que te gust√≥ el '{producto_base_para_reglas}', te sugerimos otros vestidos que son muy populares entre clientes con gustos similares a los tuyos (Cl√∫ster {nuevo_cluster_predicho}):\")\n",
    "        for prod in sugerencias_reglas: # sugerencias_reglas aqu√≠ contiene los productos del fallback\n",
    "            print(f\"- {prod}\")\n",
    "    elif tipo_sugerencia == \"general_popular\":\n",
    "        print(f\"\\n‚ú® Adem√°s del '{producto_base_para_reglas}', echa un vistazo a estos vestidos que son un √©xito en nuestro Atelier:\")\n",
    "        for prod in sugerencias_reglas: # sugerencias_reglas aqu√≠ contiene los productos generales populares\n",
    "            print(f\"- {prod}\")\n",
    "\n",
    "else:\n",
    "    print(\"No se pudo obtener la informaci√≥n actualizada del cliente para la recomendaci√≥n.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6755774a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "\n",
    "# --- SECCI√ìN 1: Carga de Datos y Generaci√≥n de Reglas de Asociaci√≥n ---\n",
    "\n",
    "try:\n",
    "    df_completo = pd.read_csv('atelier-datshet-2024-transacciones.csv')\n",
    "    print(\"DataFrame 'atelier-datshet-2024-transacciones.csv' cargado exitosamente.\")\n",
    "    \n",
    "except FileNotFoundError:\n",
    "    print(\"Error: No se encontr√≥ el archivo. Usando datos simulados para demostraci√≥n.\")\n",
    "    # Datos simulados para demostraci√≥n\n",
    "    data_simulada = {\n",
    "        'transaccion_id': [f'T{i}' for i in range(1, 201)],\n",
    "        'producto_talla': np.random.choice(['S', 'M', 'L', 'Talla √önica'], size=200),\n",
    "        'producto_color': np.random.choice(['Rojo', 'Negro', 'Azul', 'Beige', 'Verde', 'Blanco'], size=200),\n",
    "        'producto_temporada': np.random.choice(['Verano', 'Invierno', 'Primavera', 'Todo el A√±o'], size=200),\n",
    "        'vestido_estilo': np.random.choice(['Casual', 'Noche', 'C√≥ctel', 'Bohemio'], size=200),\n",
    "        'vestido_condicion': np.random.choice(['Nuevo', 'Usado'], size=200),\n",
    "        'producto_nombre': ['Vestido ' + str(i) for i in range(1, 201)],\n",
    "        'rese√±a_rating': np.random.uniform(3.5, 5.0, size=200).round(1),\n",
    "        'vestido_precio_venta': np.random.randint(500, 2500, size=200)\n",
    "    }\n",
    "    df_completo = pd.DataFrame(data_simulada)\n",
    "    df_completo.loc[0:20, ['producto_talla', 'vestido_estilo']] = ['M', 'Casual']\n",
    "    df_completo.loc[0:20, 'producto_temporada'] = 'Verano'\n",
    "\n",
    "if 'producto_id' not in df_completo.columns:\n",
    "    df_completo['producto_id'] = range(1, len(df_completo) + 1)\n",
    "\n",
    "# 2. Preprocesamiento de datos y generaci√≥n del 'basket'\n",
    "atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "df_completo.dropna(subset=['transaccion_id'] + atributos, inplace=True)\n",
    "df_completo.reset_index(drop=True, inplace=True)\n",
    "df_analisis = df_completo[['transaccion_id'] + atributos].copy()\n",
    "list_of_attributes = [df_analisis[attr].apply(lambda x: f\"{attr}_{x}\") for attr in atributos]\n",
    "items = pd.concat(list_of_attributes, ignore_index=True)\n",
    "df_basket = pd.DataFrame({\n",
    "    'transaccion_id': df_analisis['transaccion_id'].repeat(len(atributos)).reset_index(drop=True),\n",
    "    'item': items\n",
    "})\n",
    "basket = (df_basket.groupby(['transaccion_id', 'item'])['item'].count().unstack(fill_value=0).reset_index().set_index('transaccion_id'))\n",
    "basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "# 3. Filtrar y aplicar Apriori para generar reglas\n",
    "min_transactions = 5 \n",
    "product_counts = basket.sum(axis=0)\n",
    "products_to_keep = product_counts[product_counts >= min_transactions].index\n",
    "basket_filtered = basket[products_to_keep]\n",
    "basket_bool = basket_filtered.astype(bool)\n",
    "\n",
    "try:\n",
    "    frequent_itemsets = apriori(basket_bool, min_support=0.01, use_colnames=True)\n",
    "    rules_df = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "    rules_df = rules_df.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
    "    print(f\"\\nSe generaron {len(rules_df)} reglas de asociaci√≥n.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error al generar reglas: {e}. Usando un DataFrame de reglas vac√≠o.\")\n",
    "    rules_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- SECCI√ìN 2: Funciones para la Recomendaci√≥n (MEJORADA) ---\n",
    "\n",
    "def recomendar_por_similitud(atributos_elegidos, rules_df, df_transacciones_para_fallback, num_recomendaciones=2):\n",
    "    \"\"\"\n",
    "    Busca la regla m√°s similar a los atributos dados, priorizando lift y confianza.\n",
    "    \"\"\"\n",
    "    if not rules_df.empty:\n",
    "        mejores_reglas = []\n",
    "        for _, row in rules_df.iterrows():\n",
    "            antecedent = row['antecedents']\n",
    "            coincidencias = len(antecedent.intersection(atributos_elegidos))\n",
    "            if coincidencias > 0:\n",
    "                mejores_reglas.append({\n",
    "                    'regla': row,\n",
    "                    'coincidencias': coincidencias\n",
    "                })\n",
    "        \n",
    "        if mejores_reglas:\n",
    "            mejores_reglas.sort(key=lambda x: (x['coincidencias'], x['regla']['lift'], x['regla']['confidence']), reverse=True)\n",
    "            mejor_regla = mejores_reglas[0]['regla']\n",
    "            recomendaciones = list(mejor_regla['consequents'])\n",
    "            atributos_finales = [item for item in recomendaciones if item not in atributos_elegidos]\n",
    "            \n",
    "            print(\"\\n--- Diagn√≥stico: Regla m√°s similar encontrada ---\")\n",
    "            print(f\"Antecedente: {set(mejor_regla['antecedents'])}\")\n",
    "            print(f\"Consecuente: {set(mejor_regla['consequents'])}\")\n",
    "            print(f\"Lift: {mejor_regla['lift']:.2f}, Confianza: {mejor_regla['confidence']:.2f}\")\n",
    "\n",
    "            return atributos_finales[:num_recomendaciones], \"regla_mas_similar\"\n",
    "\n",
    "    print(\"üí° No se encontraron reglas de asociaci√≥n. Usando el fallback de atributos populares.\")\n",
    "    columnas_atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "    list_of_attributes = [df_transacciones_para_fallback[attr].apply(lambda x: f\"{attr}_{x}\") \n",
    "                          for attr in columnas_atributos if attr in df_transacciones_para_fallback.columns and not df_transacciones_para_fallback[attr].empty]\n",
    "    if not list_of_attributes:\n",
    "        return [], \"no_hay_datos\"\n",
    "    atributos_populares_serie = pd.concat(list_of_attributes)\n",
    "    atributos_populares = (atributos_populares_serie.value_counts().head(num_recomendaciones + len(atributos_elegidos)).index.tolist())\n",
    "    general_popular_attributes = [attr for attr in atributos_populares if attr not in atributos_elegidos]\n",
    "    return general_popular_attributes[:num_recomendaciones], \"atributos_populares\"\n",
    "\n",
    "def encontrar_vestidos_por_atributos(df_original, atributos_recomendados, excluidos=None):\n",
    "    if not atributos_recomendados: return pd.DataFrame()\n",
    "    filtros = []\n",
    "    columnas_mapeo = {\n",
    "        'producto_talla': 'producto_talla',\n",
    "        'producto_color': 'producto_color',\n",
    "        'producto_temporada': 'producto_temporada',\n",
    "        'vestido_estilo': 'vestido_estilo',\n",
    "        'vestido_condicion': 'vestido_condicion'\n",
    "    }\n",
    "    for attr_str in atributos_recomendados:\n",
    "        found_col = False\n",
    "        for prefix, col_name in columnas_mapeo.items():\n",
    "            if attr_str.startswith(f\"{prefix}_\"):\n",
    "                valor = attr_str.replace(f\"{prefix}_\", \"\", 1)\n",
    "                filtros.append(df_original[col_name] == valor)\n",
    "                found_col = True\n",
    "                break\n",
    "        if not found_col:\n",
    "            print(f\"Advertencia: El atributo '{attr_str}' no coincide con ninguna columna conocida.\")\n",
    "    if not filtros: return pd.DataFrame()\n",
    "    filtro_combinado = filtros[0]\n",
    "    for i in range(1, len(filtros)):\n",
    "        filtro_combinado &= filtros[i]\n",
    "    resultados = df_original[filtro_combinado].copy()\n",
    "    if excluidos:\n",
    "      resultados = resultados[~resultados['producto_nombre'].isin(excluidos)]\n",
    "    return resultados.head(5)\n",
    "\n",
    "\n",
    "# --- SECCI√ìN 3: PRUEBA CON TU TRANSACCI√ìN ESPEC√çFICA ---\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- PRUEBA DEL SISTEMA CON LA TRANSACCI√ìN QUE ME DISTE ---\")\n",
    "\n",
    "datos_transaccion = {\n",
    "    'producto_nombre': 'Vestido Cruzado Estampado Animal',\n",
    "    'producto_talla': 'M',\n",
    "    'producto_color': 'Multicolor',\n",
    "    'producto_temporada': 'Oto√±o, Invierno',\n",
    "    'vestido_estilo': 'Casual',\n",
    "    'vestido_condicion': 'Nuevo'\n",
    "}\n",
    "\n",
    "atributos_elegidos = frozenset([\n",
    "    f\"producto_talla_{datos_transaccion['producto_talla']}\",\n",
    "    f\"producto_color_{datos_transaccion['producto_color']}\",\n",
    "    f\"producto_temporada_{datos_transaccion['producto_temporada']}\",\n",
    "    f\"vestido_estilo_{datos_transaccion['vestido_estilo']}\",\n",
    "    f\"vestido_condicion_{datos_transaccion['vestido_condicion']}\"\n",
    "])\n",
    "\n",
    "print(f\"La transacci√≥n del cliente incluye un vestido con estos atributos:\\n{list(atributos_elegidos)}\")\n",
    "atributos_recomendados, metodo = recomendar_por_similitud(\n",
    "    atributos_elegidos=atributos_elegidos,\n",
    "    rules_df=rules_df,\n",
    "    df_transacciones_para_fallback=df_completo,\n",
    "    num_recomendaciones=3\n",
    ")\n",
    "\n",
    "print(f\"\\nEl sistema recomienda buscar vestidos con los siguientes atributos:\\n{atributos_recomendados}\")\n",
    "print(f\"(M√©todo utilizado: {metodo})\")\n",
    "\n",
    "vestidos_recomendados = encontrar_vestidos_por_atributos(\n",
    "    df_original=df_completo, \n",
    "    atributos_recomendados=atributos_recomendados,\n",
    "    excluidos=[datos_transaccion['producto_nombre']]\n",
    ")\n",
    "\n",
    "# 4. Mostramos los vestidos con todos los detalles\n",
    "if not vestidos_recomendados.empty:\n",
    "    print(\"\\n¬°Vestidos recomendados para el cliente! (coinciden con los atributos sugeridos)\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    \n",
    "    # Lista de columnas que quieres mostrar en el resultado final\n",
    "    columnas_a_mostrar = [\n",
    "        'producto_nombre',\n",
    "        'rese√±a_rating',\n",
    "        'producto_temporada',\n",
    "        'producto_talla',\n",
    "        'producto_color',\n",
    "        'vestido_estilo',\n",
    "        'vestido_condicion',\n",
    "        'vestido_precio_venta'\n",
    "    ]\n",
    "    \n",
    "       # Filtramos la lista para quedarnos solo con las columnas que existen en el DataFrame\n",
    "    columnas_existentes = [col for col in columnas_a_mostrar if col in vestidos_recomendados.columns]\n",
    "    \n",
    "    # Crea un nuevo DataFrame solo con las columnas que existen y quieres mostrar\n",
    "    df_final_recomendaciones = vestidos_recomendados[columnas_existentes]\n",
    "\n",
    "    # Muestra el DataFrame en la consola\n",
    "    print(df_final_recomendaciones)\n",
    "else:\n",
    "    print(\"\\nLo siento, no se encontraron vestidos en el cat√°logo que coincidan con los atributos recomendados.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "610c42d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reglas de asociaci√≥n cargadas desde 'rules_recomendacion.json'.\n",
      "\n",
      "==================================================\n",
      "--- PRUEBA DEL SISTEMA CON LA TRANSACCI√ìN QUE ME DISTE ---\n",
      "La transacci√≥n del cliente incluye un vestido con estos atributos:\n",
      "['producto_temporada_Todo el A√±o', 'producto_talla_XL', 'producto_color_verde', 'vestido_estilo_De Noche', 'vestido_condicion_Nuevo']\n",
      "\n",
      "--- Diagn√≥stico: Regla m√°s similar encontrada ---\n",
      "Antecedente: {'producto_temporada_Todo el A√±o'}\n",
      "Consecuente: {'producto_temporada_Invierno'}\n",
      "Lift: 5.61, Confianza: 0.79\n",
      "\n",
      "El sistema recomienda buscar vestidos con los siguientes atributos:\n",
      "['producto_temporada_Invierno']\n",
      "(M√©todo utilizado: regla_mas_similar)\n",
      "\n",
      "¬°Vestidos recomendados para el cliente! (coinciden con los atributos sugeridos)\n",
      "-------------------------------------------------------------------------\n",
      "                                producto_nombre  rese√±a_rating  \\\n",
      "17     Vestido de Punto Grueso con Cuello Cisne              5   \n",
      "22            Vestido de Punto Grueso con Rayas              4   \n",
      "47          Vestido de Punto Grueso con Capucha              1   \n",
      "96   Vestido de Punto Grueso con Cuello Redondo              5   \n",
      "117           Vestido de Punto Grueso con Rayas              4   \n",
      "\n",
      "    producto_temporada producto_talla producto_color  \n",
      "17            Invierno              M    Gris Carb√≥n  \n",
      "22            Invierno              M     Multicolor  \n",
      "47            Invierno              M          Negro  \n",
      "96            Invierno              M          Negro  \n",
      "117           Invierno              M     Multicolor  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from mlxtend.frequent_patterns import apriori, association_rules\n",
    "import numpy as np\n",
    "import os\n",
    "import json # Importamos la librer√≠a para manejar archivos JSON\n",
    "\n",
    "# --- SECCI√ìN 1: Carga de Datos y Generaci√≥n/Carga de Reglas de Asociaci√≥n ---\n",
    "\n",
    "RULES_FILE = 'rules_recomendacion.json'\n",
    "DATA_FILE = 'atelier-datshet-2024-transacciones.csv'\n",
    "df_completo = None # Inicializamos la variable para evitar errores de referencia\n",
    "\n",
    "# Primero, intentamos cargar el archivo de reglas ya generado\n",
    "try:\n",
    "    if os.path.exists(RULES_FILE):\n",
    "        rules_df = pd.read_json(RULES_FILE)\n",
    "        # Convertimos las columnas de listas (del JSON) a frozensets\n",
    "        rules_df['antecedents'] = rules_df['antecedents'].apply(frozenset)\n",
    "        rules_df['consequents'] = rules_df['consequents'].apply(frozenset)\n",
    "        print(f\"\\nReglas de asociaci√≥n cargadas desde '{RULES_FILE}'.\")\n",
    "    else:\n",
    "        raise FileNotFoundError\n",
    "except FileNotFoundError:\n",
    "    print(f\"\\nNo se encontr√≥ el archivo de reglas '{RULES_FILE}'. Generando nuevas reglas...\")\n",
    "    # Si no existe el archivo de reglas, lo generamos desde cero\n",
    "    try:\n",
    "        df_completo = pd.read_csv(DATA_FILE)\n",
    "        if 'producto_id' not in df_completo.columns:\n",
    "         df_completo['producto_id'] = range(1, len(df_completo) + 1)\n",
    "\n",
    "        print(f\"DataFrame '{DATA_FILE}' cargado exitosamente para la generaci√≥n de reglas.\")\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: No se encontr√≥ el archivo de datos '{DATA_FILE}'. Usando datos simulados para demostraci√≥n.\")\n",
    "        # Datos simulados para demostraci√≥n\n",
    "        data_simulada = {\n",
    "            'transaccion_id': [f'T{i}' for i in range(1, 201)],\n",
    "            'producto_talla': np.random.choice(['S', 'M', 'L', 'Talla √önica'], size=200),\n",
    "            'producto_color': np.random.choice(['Rojo', 'Negro', 'Azul', 'Beige', 'Verde', 'Blanco'], size=200),\n",
    "            'producto_temporada': np.random.choice(['Verano', 'Invierno', 'Primavera', 'Todo el A√±o'], size=200),\n",
    "            'vestido_estilo': np.random.choice(['Casual', 'Noche', 'C√≥ctel', 'Bohemio'], size=200),\n",
    "            'vestido_condicion': np.random.choice(['Nuevo', 'Usado'], size=200),\n",
    "            'producto_nombre': ['Vestido ' + str(i) for i in range(1, 201)],\n",
    "            'rese√±a_rating': np.random.uniform(3.5, 5.0, size=200).round(1),\n",
    "            'vestido_precio_venta': np.random.randint(500, 2500, size=200)\n",
    "        }\n",
    "        df_completo = pd.DataFrame(data_simulada)\n",
    "        df_completo.loc[0:20, ['producto_talla', 'vestido_estilo']] = ['M', 'Casual']\n",
    "        df_completo.loc[0:20, 'producto_temporada'] = 'Verano'\n",
    "\n",
    "    # 2. Preprocesamiento de datos y generaci√≥n del 'basket'\n",
    "    atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "    df_completo.dropna(subset=['transaccion_id'] + atributos, inplace=True)\n",
    "    df_completo.reset_index(drop=True, inplace=True)\n",
    "    df_analisis = df_completo[['transaccion_id'] + atributos].copy()\n",
    "    list_of_attributes = [df_analisis[attr].apply(lambda x: f\"{attr}_{x}\") for attr in atributos]\n",
    "    items = pd.concat(list_of_attributes, ignore_index=True)\n",
    "    df_basket = pd.DataFrame({\n",
    "        'transaccion_id': df_analisis['transaccion_id'].repeat(len(atributos)).reset_index(drop=True),\n",
    "        'item': items\n",
    "    })\n",
    "    basket = (df_basket.groupby(['transaccion_id', 'item'])['item'].count().unstack(fill_value=0).reset_index().set_index('transaccion_id'))\n",
    "    basket = basket.applymap(lambda x: 1 if x > 0 else 0)\n",
    "\n",
    "    # 3. Filtrar y aplicar Apriori para generar reglas\n",
    "    min_transactions = 5 \n",
    "    product_counts = basket.sum(axis=0)\n",
    "    products_to_keep = product_counts[product_counts >= min_transactions].index\n",
    "    basket_filtered = basket[products_to_keep]\n",
    "    basket_bool = basket_filtered.astype(bool)\n",
    "\n",
    "    try:\n",
    "        frequent_itemsets = apriori(basket_bool, min_support=0.01, use_colnames=True)\n",
    "        rules_df = association_rules(frequent_itemsets, metric=\"lift\", min_threshold=1.2)\n",
    "        rules_df = rules_df.sort_values(['lift', 'confidence'], ascending=[False, False])\n",
    "        print(f\"\\nSe generaron {len(rules_df)} reglas de asociaci√≥n.\")\n",
    "        # Preparamos el DataFrame para guardarlo en JSON\n",
    "        # Convertimos frozensets a listas para que puedan serializarse\n",
    "        rules_df['antecedents'] = rules_df['antecedents'].apply(list)\n",
    "        rules_df['consequents'] = rules_df['consequents'].apply(list)\n",
    "        rules_df.to_json(RULES_FILE, orient='records', indent=4)\n",
    "        print(f\"Reglas guardadas en '{RULES_FILE}'.\")\n",
    "        \n",
    "        # ‚úÖ Guardar modelos auxiliares en .pkl\n",
    "        joblib.dump(rules_df, 'rules.pkl')\n",
    "        joblib.dump(frequent_itemsets, 'frequent_itemsets.pkl')\n",
    "        joblib.dump(basket_filtered, 'basket_filtered.pkl')\n",
    "        print(\"Modelos exportados como archivos .pkl.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error al generar reglas: {e}. Usando un DataFrame de reglas vac√≠o.\")\n",
    "        rules_df = pd.DataFrame()\n",
    "\n",
    "\n",
    "# --- SECCI√ìN 2: Funciones para la Recomendaci√≥n (MEJORADA) ---\n",
    "\n",
    "def recomendar_por_similitud(atributos_elegidos, rules_df, df_transacciones_para_fallback, num_recomendaciones=2):\n",
    "    \"\"\"\n",
    "    Busca la regla m√°s similar a los atributos dados, priorizando lift y confianza.\n",
    "    \"\"\"\n",
    "    if not rules_df.empty:\n",
    "        mejores_reglas = []\n",
    "        for _, row in rules_df.iterrows():\n",
    "            antecedent = row['antecedents']\n",
    "            coincidencias = len(antecedent.intersection(atributos_elegidos))\n",
    "            if coincidencias > 0:\n",
    "                mejores_reglas.append({\n",
    "                    'regla': row,\n",
    "                    'coincidencias': coincidencias\n",
    "                })\n",
    "        \n",
    "        if mejores_reglas:\n",
    "            mejores_reglas.sort(key=lambda x: (x['coincidencias'], x['regla']['lift'], x['regla']['confidence']), reverse=True)\n",
    "            mejor_regla = mejores_reglas[0]['regla']\n",
    "            recomendaciones = list(mejor_regla['consequents'])\n",
    "            atributos_finales = [item for item in recomendaciones if item not in atributos_elegidos]\n",
    "            \n",
    "            print(\"\\n--- Diagn√≥stico: Regla m√°s similar encontrada ---\")\n",
    "            print(f\"Antecedente: {set(mejor_regla['antecedents'])}\")\n",
    "            print(f\"Consecuente: {set(mejor_regla['consequents'])}\")\n",
    "            print(f\"Lift: {mejor_regla['lift']:.2f}, Confianza: {mejor_regla['confidence']:.2f}\")\n",
    "\n",
    "            return atributos_finales[:num_recomendaciones], \"regla_mas_similar\"\n",
    "\n",
    "    print(\"üí° No se encontraron reglas de asociaci√≥n. Usando el fallback de atributos populares.\")\n",
    "    columnas_atributos = ['producto_talla', 'producto_color', 'producto_temporada', 'vestido_estilo', 'vestido_condicion']\n",
    "    list_of_attributes = [df_transacciones_para_fallback[attr].apply(lambda x: f\"{attr}_{x}\") \n",
    "                          for attr in columnas_atributos if attr in df_transacciones_para_fallback.columns and not df_transacciones_para_fallback[attr].empty]\n",
    "    if not list_of_attributes:\n",
    "        return [], \"no_hay_datos\"\n",
    "    atributos_populares_serie = pd.concat(list_of_attributes)\n",
    "    atributos_populares = (atributos_populares_serie.value_counts().head(num_recomendaciones + len(atributos_elegidos)).index.tolist())\n",
    "    general_popular_attributes = [attr for attr in atributos_populares if attr not in atributos_elegidos]\n",
    "    return general_popular_attributes[:num_recomendaciones], \"atributos_populares\"\n",
    "\n",
    "def encontrar_vestidos_por_atributos(df_original, atributos_recomendados, excluidos=None):\n",
    "    if not atributos_recomendados: return pd.DataFrame()\n",
    "    filtros = []\n",
    "    columnas_mapeo = {\n",
    "        'producto_talla': 'producto_talla',\n",
    "        'producto_color': 'producto_color',\n",
    "        'producto_temporada': 'producto_temporada',\n",
    "        'vestido_estilo': 'vestido_estilo',\n",
    "        'vestido_condicion': 'vestido_condicion'\n",
    "    }\n",
    "    for attr_str in atributos_recomendados:\n",
    "        found_col = False\n",
    "        for prefix, col_name in columnas_mapeo.items():\n",
    "            if attr_str.startswith(f\"{prefix}_\"):\n",
    "                valor = attr_str.replace(f\"{prefix}_\", \"\", 1)\n",
    "                filtros.append(df_original[col_name] == valor)\n",
    "                found_col = True\n",
    "                break\n",
    "        if not found_col:\n",
    "            print(f\"Advertencia: El atributo '{attr_str}' no coincide con ninguna columna conocida.\")\n",
    "    if not filtros: return pd.DataFrame()\n",
    "    filtro_combinado = filtros[0]\n",
    "    for i in range(1, len(filtros)):\n",
    "        filtro_combinado &= filtros[i]\n",
    "    resultados = df_original[filtro_combinado].copy()\n",
    "    if excluidos:\n",
    "      resultados = resultados[~resultados['producto_nombre'].isin(excluidos)]\n",
    "    return resultados.head(5)\n",
    "\n",
    "\n",
    "# --- SECCI√ìN 3: PRUEBA CON TU TRANSACCI√ìN ESPEC√çFICA ---\n",
    "# Cargamos el DataFrame completo para la funci√≥n de b√∫squeda de vestidos y fallback\n",
    "if df_completo is None:\n",
    "    try:\n",
    "        df_completo = pd.read_csv(DATA_FILE)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"\\nAdvertencia: El archivo de datos '{DATA_FILE}' no se encontr√≥ para la b√∫squeda final. Usando datos simulados.\")\n",
    "        df_completo = pd.DataFrame(data_simulada)\n",
    "\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"--- PRUEBA DEL SISTEMA CON LA TRANSACCI√ìN QUE ME DISTE ---\")\n",
    "\n",
    "datos_transaccion = {\n",
    "    'producto_nombre': 'Vestido de Noche Meirius Largo Verde con Hombro Descubierto',\n",
    "    'producto_talla': 'XL',\n",
    "    'producto_color': 'verde',\n",
    "    'producto_temporada': 'Todo el A√±o',\n",
    "    'vestido_estilo': 'De Noche',\n",
    "    'vestido_condicion': 'Nuevo',\n",
    "}\n",
    "\n",
    "atributos_elegidos = frozenset([\n",
    "    f\"producto_talla_{datos_transaccion['producto_talla']}\",\n",
    "    f\"producto_color_{datos_transaccion['producto_color']}\",\n",
    "    f\"producto_temporada_{datos_transaccion['producto_temporada']}\",\n",
    "    f\"vestido_estilo_{datos_transaccion['vestido_estilo']}\",\n",
    "    f\"vestido_condicion_{datos_transaccion['vestido_condicion']}\"\n",
    "])\n",
    "\n",
    "print(f\"La transacci√≥n del cliente incluye un vestido con estos atributos:\\n{list(atributos_elegidos)}\")\n",
    "atributos_recomendados, metodo = recomendar_por_similitud(\n",
    "    atributos_elegidos=atributos_elegidos,\n",
    "    rules_df=rules_df,\n",
    "    df_transacciones_para_fallback=df_completo,\n",
    "    num_recomendaciones=3\n",
    ")\n",
    "\n",
    "print(f\"\\nEl sistema recomienda buscar vestidos con los siguientes atributos:\\n{atributos_recomendados}\")\n",
    "print(f\"(M√©todo utilizado: {metodo})\")\n",
    "\n",
    "vestidos_recomendados = encontrar_vestidos_por_atributos(\n",
    "    df_original=df_completo,\n",
    "    atributos_recomendados=atributos_recomendados,\n",
    "    excluidos=[datos_transaccion['producto_nombre']]\n",
    ")\n",
    "\n",
    "if not vestidos_recomendados.empty:\n",
    "    print(\"\\n¬°Vestidos recomendados para el cliente! (coinciden con los atributos sugeridos)\")\n",
    "    print(\"-------------------------------------------------------------------------\")\n",
    "    \n",
    "    columnas_a_mostrar = [\n",
    "        'producto_id',\n",
    "        'producto_nombre',\n",
    "        'rese√±a_rating',\n",
    "        'producto_temporada',\n",
    "        'producto_talla',\n",
    "        'producto_color',\n",
    "        'vestido_estilo',\n",
    "        'vestido_condicion',\n",
    "        'vestido_precio_venta'\n",
    "    ]\n",
    "    \n",
    "    columnas_existentes = [col for col in columnas_a_mostrar if col in vestidos_recomendados.columns]\n",
    "    \n",
    "    df_final_recomendaciones = vestidos_recomendados[columnas_existentes]\n",
    "    print(df_final_recomendaciones)\n",
    "else:\n",
    "    print(\"\\nLo siento, no se encontraron vestidos en el cat√°logo que coincidan con los atributos recomendados.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
